{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Next word predictor with USE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeming99/next_word_predictor/blob/master/Next_word_predictor_with_USE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAuqkDdNEzf6",
        "colab_type": "text"
      },
      "source": [
        "### Fetch and preprocess corpus - Lower case and remove punctionations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmkrPZD6E2X3",
        "colab_type": "code",
        "outputId": "26433e88-c5aa-4e7f-deaa-3fa8dee64da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "from keras.utils.data_utils import get_file\n",
        "import string\n",
        "print('\\nFetching the text...')\n",
        "url = 'https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt'\n",
        "path = get_file('arxiv_abstracts.txt', origin=url)\n",
        "\n",
        "print('\\nPreparing the sentences...')\n",
        "max_sentence_len = 40\n",
        "with open(path) as file_:\n",
        "  docs = file_.readlines()\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "# for doc in docs:\n",
        "#   print(doc.lower().translate(translator))\n",
        "sentences = [doc.lower().translate(translator) for doc in docs]\n",
        "print('First sentence: ', sentences[0])\n",
        "print('Num sentences:', len(sentences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fetching the text...\n",
            "\n",
            "Preparing the sentences...\n",
            "First sentence:  in science and engineering intelligent processing of complex signals such as images sound or language is often performed by a parameterized hierarchy of nonlinear processing layers sometimes biologically inspired hierarchical systems or more generally nested systems offer a way to generate complex mappings using simple stages each layer performs a different operation and achieves an ever more sophisticated representation of the input as for example in an deep artificial neural network an object recognition cascade in computer vision or a speech frontend processing joint estimation of the parameters of all the layers and selection of an optimal architecture is widely considered to be a difficult numerical nonconvex optimization problem difficult to parallelize for execution in a distributed computation environment and requiring significant human expert effort which leads to suboptimal systems in practice we describe a general mathematical strategy to learn the parameters and to some extent the architecture of nested systems called the method of auxiliary coordinates mac this replaces the original problem involving a deeply nested function with a constrained problem involving a different function in an augmented space without nesting the constrained problem may be solved with penaltybased methods using alternating optimization over the parameters and the auxiliary coordinates mac has provable convergence is easy to implement reusing existing algorithms for single layers can be parallelized trivially and massively applies even when parameter derivatives are not available or not desirable and is competitive with stateoftheart nonlinear optimizers even in the serial computation setting often providing reasonable models within a few iterations\n",
            "\n",
            "Num sentences: 7200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng2so3HHdxP_",
        "colab_type": "text"
      },
      "source": [
        "### Set output dictionary and array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwx13xIxdt5X",
        "colab_type": "code",
        "outputId": "df4b1180-4084-4213-ee8a-262744146839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocab_arr = list(set(' '.join(sentences).replace('\\n','').split(' ')))\n",
        "vocab_index_dict = {}\n",
        "for i, vocab in enumerate(vocab_arr):\n",
        "  vocab_index_dict[vocab] = i\n",
        "print(len(vocab_arr))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPjFYD1EedQt",
        "colab_type": "text"
      },
      "source": [
        "###Split into X and Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfgtaGCqehIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "import tensorflow.keras.backend as K\n",
        "X=[]\n",
        "y=[]\n",
        "for sent in sentences:\n",
        "  words = sent.replace('\\n','').split(' ')\n",
        "  X.append(' '.join(words[:-1]))\n",
        "  # y.append(vocab_arr.index(words[-1]))\n",
        "  tmpy = [0 for i in range(len(vocab_arr))]\n",
        "  tmpy[vocab_index_dict[words[-1]]] = 1\n",
        "  y.append(tmpy) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH8XTYL-e6PU",
        "colab_type": "text"
      },
      "source": [
        "### Split into test-train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CzM4lEDe9Ph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "# y_train = K.constant(numpy.array(y_train))\n",
        "# y_test = K.constant(numpy.array(y_test))\n",
        "y_test = numpy.array(y_test)\n",
        "y_train = numpy.array(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHEYZDaMHFsQ",
        "colab_type": "text"
      },
      "source": [
        "### Vectorize sentences using Universal Sentence Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2AkLBz1aXu7",
        "colab_type": "code",
        "outputId": "5c0ecbc3-05de-40dc-ab32-582a813afef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from absl import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
        "use_model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)\n",
        "def embed(input):\n",
        "  return use_model(input)\n",
        "\n",
        "# print(model.variables)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtmuTMYiE3fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# message_embeddings = embed(sentences)\n",
        "\n",
        "# for i, message_embedding in enumerate(np.array(message_embeddings).tolist()[:5]):\n",
        "#   print(\"Message: {}\".format(sentences[i]))\n",
        "#   print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "#   message_embedding_snippet = \", \".join(\n",
        "#       (str(x) for x in message_embedding[:3]))\n",
        "#   print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n",
        "\n",
        "X_train = embed(X_train)\n",
        "X_test = embed(X_test)\n",
        "X_train = X_train.numpy()\n",
        "X_test = X_test.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kke2me8_jD0W",
        "colab_type": "code",
        "outputId": "9f675482-b69c-46d5-a46d-b2ae09d68ad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(X_train.shape, X_test.shape, y_test.shape, y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5400, 512) (1800, 512) (1800, 2694) (5400, 2694)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpAJSlKqa0sV",
        "colab_type": "text"
      },
      "source": [
        "### Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r2_5OWzGt6q",
        "colab_type": "code",
        "outputId": "fec33414-48cf-4eaf-9d1d-d63a64528140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "from keras.callbacks import LambdaCallback\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "\n",
        "# feed into LSTM\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "embedding_size = 512\n",
        "vocab_size = len(vocab_arr)\n",
        "model = Sequential()\n",
        "# model.add(Embedding(input_dim=len(vocab_arr), output_dim=100))\n",
        "# model.add(LSTM(units=100))\n",
        "# model.add(Dense(units=2762, activation = 'softmax'))\n",
        "# model.add(Dense(units=vocab_size))\n",
        "# model.add(Activation('softmax'))\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "model = Sequential()\n",
        "# model.add(LSTM(units=100, input_shape=[512]))\n",
        "model.add(Dense(512, input_shape=[embedding_size], activation = 'relu'))\n",
        "model.add(Dense(units=vocab_size, activation = 'softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_23 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 2694)              1382022   \n",
            "=================================================================\n",
            "Total params: 1,644,678\n",
            "Trainable params: 1,644,678\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44EKAWrNa7d-",
        "colab_type": "text"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1nl-3L7Yvj2",
        "colab_type": "code",
        "outputId": "d784f185-aeb4-4fec-cae3-ebafa4bf5699",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train model\n",
        "from keras.callbacks import LambdaCallback\n",
        "import numpy as np\n",
        "def generate_next(text, num_generated=1):\n",
        "  for i in range(num_generated):\n",
        "    prediction = model.predict(x=embed([text]).numpy())\n",
        "    idx = np.argmax(prediction[-1])\n",
        "    text += ' ' + vocab_arr[idx]\n",
        "  return text\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "  print('\\nGenerating text after epoch: %d' % epoch)\n",
        "  texts = [\n",
        "    'deep convolutional',\n",
        "    'simple and effective',\n",
        "    'a nonconvex',\n",
        "    'a',\n",
        "  ]\n",
        "  for text in texts:\n",
        "    sample = generate_next(text)\n",
        "    print('%s... -> %s' % (text, sample))\n",
        "\n",
        "# print(y_train, X_train)\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=512,\n",
        "          shuffle=True,\n",
        "          epochs=20,\n",
        "          validation_data=(X_test, y_test),\n",
        "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5400 samples, validate on 1800 samples\n",
            "Epoch 1/20\n",
            "5400/5400 [==============================] - 2s 298us/step - loss: 7.6598 - acc: 0.1456 - val_loss: 7.0456 - val_acc: 0.1783\n",
            "\n",
            "Generating text after epoch: 0\n",
            "deep convolutional... -> deep convolutional performance\n",
            "simple and effective... -> simple and effective experiments\n",
            "a nonconvex... -> a nonconvex networks\n",
            "a... -> a improvement\n",
            "Epoch 2/20\n",
            "5400/5400 [==============================] - 2s 289us/step - loss: 5.9549 - acc: 0.1531 - val_loss: 4.5174 - val_acc: 0.0817\n",
            "\n",
            "Generating text after epoch: 1\n",
            "deep convolutional... -> deep convolutional performance\n",
            "simple and effective... -> simple and effective experiments\n",
            "a nonconvex... -> a nonconvex networks\n",
            "a... -> a improvement\n",
            "Epoch 3/20\n",
            "5400/5400 [==============================] - 2s 284us/step - loss: 4.2203 - acc: 0.0900 - val_loss: 4.0747 - val_acc: 0.1117\n",
            "\n",
            "Generating text after epoch: 2\n",
            "deep convolutional... -> deep convolutional networks\n",
            "simple and effective... -> simple and effective experiments\n",
            "a nonconvex... -> a nonconvex networks\n",
            "a... -> a improvement\n",
            "Epoch 4/20\n",
            "5400/5400 [==============================] - 1s 276us/step - loss: 3.9753 - acc: 0.1250 - val_loss: 3.8561 - val_acc: 0.2244\n",
            "\n",
            "Generating text after epoch: 3\n",
            "deep convolutional... -> deep convolutional networks\n",
            "simple and effective... -> simple and effective experiments\n",
            "a nonconvex... -> a nonconvex networks\n",
            "a... -> a improvement\n",
            "Epoch 5/20\n",
            "5400/5400 [==============================] - 2s 279us/step - loss: 3.7465 - acc: 0.2233 - val_loss: 3.5980 - val_acc: 0.3011\n",
            "\n",
            "Generating text after epoch: 4\n",
            "deep convolutional... -> deep convolutional networks\n",
            "simple and effective... -> simple and effective learning\n",
            "a nonconvex... -> a nonconvex networks\n",
            "a... -> a improvement\n",
            "Epoch 6/20\n",
            "5400/5400 [==============================] - 2s 282us/step - loss: 3.4463 - acc: 0.3524 - val_loss: 3.2527 - val_acc: 0.4322\n",
            "\n",
            "Generating text after epoch: 5\n",
            "deep convolutional... -> deep convolutional networks\n",
            "simple and effective... -> simple and effective models\n",
            "a nonconvex... -> a nonconvex networks\n",
            "a... -> a improvement\n",
            "Epoch 7/20\n",
            "5400/5400 [==============================] - 2s 282us/step - loss: 3.0712 - acc: 0.4635 - val_loss: 2.8477 - val_acc: 0.5894\n",
            "\n",
            "Generating text after epoch: 6\n",
            "deep convolutional... -> deep convolutional performance\n",
            "simple and effective... -> simple and effective learning\n",
            "a nonconvex... -> a nonconvex networks\n",
            "a... -> a improvement\n",
            "Epoch 8/20\n",
            "5400/5400 [==============================] - 2s 283us/step - loss: 2.6390 - acc: 0.7411 - val_loss: 2.3989 - val_acc: 0.8017\n",
            "\n",
            "Generating text after epoch: 7\n",
            "deep convolutional... -> deep convolutional performance\n",
            "simple and effective... -> simple and effective learning\n",
            "a nonconvex... -> a nonconvex networks\n",
            "a... -> a improvement\n",
            "Epoch 9/20\n",
            "5400/5400 [==============================] - 2s 290us/step - loss: 2.1749 - acc: 0.8831 - val_loss: 1.9321 - val_acc: 0.9422\n",
            "\n",
            "Generating text after epoch: 8\n",
            "deep convolutional... -> deep convolutional performance\n",
            "simple and effective... -> simple and effective learning\n",
            "a nonconvex... -> a nonconvex networks\n",
            "a... -> a improvement\n",
            "Epoch 10/20\n",
            "5400/5400 [==============================] - 2s 283us/step - loss: 1.7156 - acc: 0.9665 - val_loss: 1.4917 - val_acc: 0.9889\n",
            "\n",
            "Generating text after epoch: 9\n",
            "deep convolutional... -> deep convolutional performance\n",
            "simple and effective... -> simple and effective estimators\n",
            "a nonconvex... -> a nonconvex networks\n",
            "a... -> a improvement\n",
            "Epoch 11/20\n",
            "5400/5400 [==============================] - 2s 286us/step - loss: 1.2910 - acc: 0.9904 - val_loss: 1.1110 - val_acc: 0.9889\n",
            "\n",
            "Generating text after epoch: 10\n",
            "deep convolutional... -> deep convolutional performance\n",
            "simple and effective... -> simple and effective learning\n",
            "a nonconvex... -> a nonconvex networks\n",
            "a... -> a improvement\n",
            "Epoch 12/20\n",
            "5400/5400 [==============================] - 2s 282us/step - loss: 0.9392 - acc: 0.9920 - val_loss: 0.7975 - val_acc: 1.0000\n",
            "\n",
            "Generating text after epoch: 11\n",
            "deep convolutional... -> deep convolutional performance\n",
            "simple and effective... -> simple and effective estimators\n",
            "a nonconvex... -> a nonconvex labels\n",
            "a... -> a improvement\n",
            "Epoch 13/20\n",
            "5400/5400 [==============================] - 2s 284us/step - loss: 0.6668 - acc: 0.9930 - val_loss: 0.5687 - val_acc: 1.0000\n",
            "\n",
            "Generating text after epoch: 12\n",
            "deep convolutional... -> deep convolutional performance\n",
            "simple and effective... -> simple and effective estimators\n",
            "a nonconvex... -> a nonconvex labels\n",
            "a... -> a improvement\n",
            "Epoch 14/20\n",
            "5400/5400 [==============================] - 2s 285us/step - loss: 0.4757 - acc: 1.0000 - val_loss: 0.4125 - val_acc: 1.0000\n",
            "\n",
            "Generating text after epoch: 13\n",
            "deep convolutional... -> deep convolutional performance\n",
            "simple and effective... -> simple and effective estimators\n",
            "a nonconvex... -> a nonconvex labels\n",
            "a... -> a improvement\n",
            "Epoch 15/20\n",
            "5400/5400 [==============================] - 2s 285us/step - loss: 0.3436 - acc: 1.0000 - val_loss: 0.3011 - val_acc: 1.0000\n",
            "\n",
            "Generating text after epoch: 14\n",
            "deep convolutional... -> deep convolutional performance\n",
            "simple and effective... -> simple and effective estimators\n",
            "a nonconvex... -> a nonconvex labels\n",
            "a... -> a improvement\n",
            "Epoch 16/20\n",
            "5400/5400 [==============================] - 2s 288us/step - loss: 0.2573 - acc: 1.0000 - val_loss: 0.2311 - val_acc: 1.0000\n",
            "\n",
            "Generating text after epoch: 15\n",
            "deep convolutional... -> deep convolutional performance\n",
            "simple and effective... -> simple and effective estimators\n",
            "a nonconvex... -> a nonconvex labels\n",
            "a... -> a improvement\n",
            "Epoch 17/20\n",
            "5400/5400 [==============================] - 2s 279us/step - loss: 0.1980 - acc: 1.0000 - val_loss: 0.1817 - val_acc: 1.0000\n",
            "\n",
            "Generating text after epoch: 16\n",
            "deep convolutional... -> deep convolutional performance\n",
            "simple and effective... -> simple and effective estimators\n",
            "a nonconvex... -> a nonconvex labels\n",
            "a... -> a improvement\n",
            "Epoch 18/20\n",
            "5400/5400 [==============================] - 1s 275us/step - loss: 0.1574 - acc: 1.0000 - val_loss: 0.1464 - val_acc: 1.0000\n",
            "\n",
            "Generating text after epoch: 17\n",
            "deep convolutional... -> deep convolutional retrieval\n",
            "simple and effective... -> simple and effective estimators\n",
            "a nonconvex... -> a nonconvex labels\n",
            "a... -> a improvement\n",
            "Epoch 19/20\n",
            "5400/5400 [==============================] - 1s 271us/step - loss: 0.1286 - acc: 1.0000 - val_loss: 0.1214 - val_acc: 1.0000\n",
            "\n",
            "Generating text after epoch: 18\n",
            "deep convolutional... -> deep convolutional retrieval\n",
            "simple and effective... -> simple and effective estimators\n",
            "a nonconvex... -> a nonconvex labels\n",
            "a... -> a improvement\n",
            "Epoch 20/20\n",
            "5400/5400 [==============================] - 1s 275us/step - loss: 0.1072 - acc: 1.0000 - val_loss: 0.1024 - val_acc: 1.0000\n",
            "\n",
            "Generating text after epoch: 19\n",
            "deep convolutional... -> deep convolutional retrieval\n",
            "simple and effective... -> simple and effective estimators\n",
            "a nonconvex... -> a nonconvex labels\n",
            "a... -> a improvement\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb32ddcfa20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsgjkk24aX_H",
        "colab_type": "text"
      },
      "source": [
        "### Mount GDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4WcB7NEaYaR",
        "colab_type": "code",
        "outputId": "2265e433-0bf0-493f-944a-1f8c65ac838b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSLR_g2GZk2t",
        "colab_type": "text"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZpV5LNhT1ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/gdrive/My Drive/pencil/next_word_predictor\"\n",
        "model.save(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVg2izkvZ7ui",
        "colab_type": "text"
      },
      "source": [
        "### Save vocab_arr\n",
        "Remember to save model as well as array to interprete our model output!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDq8ev2hU0T-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_arr = numpy.array(vocab_arr)\n",
        "numpy.save('/content/gdrive/My Drive/pencil/vocab_arr.npy', vocab_arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWjdteXDaIdM",
        "colab_type": "text"
      },
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1dwafpEUJ1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "path = \"/content/gdrive/My Drive/pencil/next_word_predictor\"\n",
        "model = keras.models.load_model(path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_1holT5a2Eu",
        "colab_type": "text"
      },
      "source": [
        "### Load vocab_arr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN2fztK6a5gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_arr = numpy.load('/content/gdrive/My Drive/pencil/vocab_arr.npy') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KigcnamTZhUt",
        "colab_type": "text"
      },
      "source": [
        "### Use Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTjWCjJKiCRg",
        "colab_type": "code",
        "outputId": "0a27bcb8-0cec-4804-b6de-a3367b0b9f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocab_arr[np.argmax(model.predict(embed(['test']).numpy())[-1])]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'datasets'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxHdHFdgZpWS",
        "colab_type": "code",
        "outputId": "41055e52-3c7c-40de-914a-600e66f564b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocab_arr[np.argmax(model.predict(embed(['fly']).numpy())[-1])]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'improvement'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnZ0MTkyZppX",
        "colab_type": "code",
        "outputId": "3ad3cc3e-ad0c-4890-adee-3536939248b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocab_arr[np.argmax(model.predict(embed(['datasets']).numpy())[-1])]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'work'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G3Vd-GLar7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}